<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Tom Preston-Werner</title>
 <link href="http://daidong.github.io/atom.xml" rel="self"/>
 <link href="http://daidong.github.io"/>
 <updated>2014-02-13T21:09:15-06:00</updated>
 <id>http://tom.preston-werner.com/</id>
 <author>
   <name>Dai Dong</name>
   <email>daidongly@gmail.com</email>
 </author>

 
 <entry>
   <title>SC Read List In Last 5 years</title>
   <link href="http://daidong.github.io/2014/02/12/sc-read-list.html"/>
   <updated>2014-02-12T00:00:00-06:00</updated>
   <id>http://daidong.github.io/blog/2014/02/12/sc-read-list</id>
   <content type="html">&lt;h1&gt;SC Read List In Last 5 years&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;12 Feb. 2014 - Lubbock&lt;/p&gt;


&lt;p&gt;In this post, we are trying to go through all the papers in SC in last 5 years
(2008 - 2013) to see the hot topics about high performance I/O or storage systems. The fields
we actually care about in this post only contain: High performance file systems,
 memory storages, solid storage devices. This is a huge project because there
are about hundreds of papers each year and lots of them are relevant with I/Os.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In 2008, people are still talking about &quot;entering the petaflop era&quot; and finally
acheive the 1.38 Pflop/s peek (double precision) performance. It is
funny that, only three or four years later, the &quot;petaflop era&quot; has passed and
the roadmap for &quot;exscale&quot; is comming.&lt;/p&gt;

&lt;p&gt;There are some papers describing architecture, experience, applications on the first several generation &quot;PetaScale&quot; computers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Kevin J. Barker et al.] Entering the petaflop era: the architecture and performance of Roadrunner&lt;/li&gt;
&lt;li&gt;[S. Alam et al.] Early evaluation of IBM BlueGene/P&lt;/li&gt;
&lt;li&gt;[Gregory L. Lee et al.] Lessons learned at 208K: towards debugging millions of cores&lt;/li&gt;
&lt;li&gt;[Heshan Lin et al.] Massively parallel genomic sequence search on the Blue Gene/P architecture&lt;/li&gt;
&lt;li&gt;[Subhash Saini et al.] Scientific application-based performance comparison of SGI Altix 4700, IBM POWER5+, and SGI ICE 8200 supercomputers&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Two papers from Wei-Keng Liao and Alok Choudhary group about caching:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Wei-keng Liao et al.] Dynamically adapting file domain partitioning methods for collective I/O based on underlying parallel file system locking protocols&lt;/li&gt;
&lt;li&gt;[Arifa Nisar, Wei-keng Liao, et al.] Scaling parallel I/O performance through I/O delegate and caching system&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Also there are two papers from Yong Chen and XianHe Sun group about Prefetching:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Yong Chen et al.] Hiding I/O latency with pre-execution prefetching for parallel applications&lt;/li&gt;
&lt;li&gt;[Serendra Byna et al.] Parallel I/O prefetching using MPI file caching and I/O signatures&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Others:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Philip H. Carns et al.] Using server-to-server communication in parallel file systems to simplify consistency and improve performance&lt;/li&gt;
&lt;li&gt;[Ryutaro Susukita et al.] Performance prediction of large-scale parallell system and application using macro-level simulation&lt;/li&gt;
&lt;li&gt;[Hongzhang Shan et al.] Characterizing and predicting the I/O performance of HPC applications using a parameterized synthetic benchmark&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It is interesting that in this year, people are more concentrating on using the new petaflop machines: different
applications are immigrated to those new machines and new strategies are proposed to perdict and characterize
their performance. In storage? the caching and prefetching are still the core topics.  There are actually not so many papers about some very popular topics which are proven to
dominate this conference in next several years: GPU, Cloud (Virtualization, MapReduce, HDFS), SSD and Memory-storage.
 It is interesting to see how these things became hotter and hotter. You can access the proceeding of 2008 SC
from here: &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1413370&amp;amp;picked=prox&quot;&gt;SC 2008 Proceeding&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In 2009, multi-core is hot here. It is so clear to see the transfer of interests from next generation
high performance machines to the so-called multi/many cores systems or on-chip systems. At the same
time, there are not many papers about storage systems. I listed them following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Yu Hua et al.] SmartStore: a new metadata organization paradigm with semantic-awareness for next-generation file systems&lt;/li&gt;
&lt;li&gt;[John Bent et al.] PLFS: a checkpoint filesystem for parallel applications&lt;/li&gt;
&lt;li&gt;[Jing Xing et al.] Adaptive and scalable metadata management to support a trillion files&lt;/li&gt;
&lt;li&gt;[Samuel Lang et al.] I/O performance challenges at leadership scale&lt;/li&gt;
&lt;li&gt;[Shih-wei Liao et al.] Machine learning-based prefetch optimization for data center applications&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Among these papers, PLFS is a pretty outstanding one here. Although it is not well accepted till today (2014), but
it does affect lots of parallel filesystems. Groups from ANL with Rob Ross have several papers in this
meeting including:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Samuel Lang et al.] I/O performance challenges at leadership scale&lt;/li&gt;
&lt;li&gt;[Wesley Kendall et al.] Terascale data organization for discovering multivariate climatic trends&lt;/li&gt;
&lt;li&gt;[Tom Peterka et al.] A configurable algorithm for parallel image-compositing applications&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;These results are generated right after the IBM Blue/P (Intrepid) has been installed at ANL in 2008, when
it got rank #3 on the June Top 500 list &lt;a href=&quot;http://en.wikipedia.org/wiki/Blue_Gene&quot;&gt;Wiki&lt;/a&gt;. This is also part of the reason why they are able to write a
paper to talk about the I/O performance challenges at leadership scale in a very general way. It is a
pity that even China has lanched its TianHe-1 and TianHe-2 in last two years, there still not enough
papers about them in such conferences like SC. You can access the proceeding of 2009 from here: &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1654059&amp;amp;picked=prox&quot;&gt;SC 2009 Proceeding&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In 2010, there is a blooming on flash-based storage systems and GPUs. In general, people began to notice
flash can be well used in high performance clusters. The reason is clear: disks are not fast enough for current
applications and future &quot;Exscale&quot; applications. We list the papers about flash-based or any other new
device-based storage papers here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Adrian M. Caulfield, et al.] &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1884660&amp;amp;dl=ACM&amp;amp;coll=DL&amp;amp;CFID=271517975&amp;amp;CFTOKEN=30780424&quot;&gt;Understanding the Impact of Emerging Non-Volatile Memories on High-Performance, IO-Intensive Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[Jiahua He, et al.] &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1884661&amp;amp;dl=ACM&amp;amp;coll=DL&amp;amp;CFID=271517975&amp;amp;CFTOKEN=30780424&quot;&gt;DASH: a Recipe for a Flash-based Data Intensive Supercomputer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;There are also several papers about I/O systems as following list shows. The first paper &quot;I/O forwarding&quot;
came from ANL (Rob's group); it is still based on the IBM Blue Gene/P System, which is  currently (2010)
still one of the most fastest &quot;Petaflop&quot; super-computers. In fact, in this year, only from the paper name,
we can see bunch of papers based on this IBM Blue Gene/P computers. Back to the storage systems, we can
easily to see that the core topic of this year is coordination among lots of storage servers.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Venkatram Vishwanath, et al.] &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1884678&amp;amp;dl=ACM&amp;amp;coll=DL&amp;amp;CFID=271517975&amp;amp;CFTOKEN=30780424&quot;&gt;Accelerating I/O Forwarding in IBM Blue Gene/P Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[Jay Lofstead, et al.] Managing Variability in the IO Performance of Petascale Storage Systems&lt;/li&gt;
&lt;li&gt;[Xuecheng Zhang, et al.] IOrchestrator: Improving the Performance of Multi-node I/O Systems via Inter-Server Coordination&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;You can access the proceedings of 2010 from here: &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=1884643&amp;amp;picked=prox&quot;&gt;SC 2010 Proceeding&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In 2011, there is a standalone session named &quot;coordinating I/O&quot; which is pretty the continuation of last
year's path. We list all the papers in this section:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Huaiming Song, et al.] Server-side I/O coordination for parallel file systems&lt;/li&gt;
&lt;li&gt;[Xuechen Zhang, et al.] QoS support for end users of I/O-intensive applications using shared storage systems&lt;/li&gt;
&lt;li&gt;[Venkatram Vishwanath, et al.] Topology-aware data movement and staging for I/O acceleration on Blue Gene/P supercomputing systems&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We can see these old faces showing again in this year's SC conference. Venkatram works on I/O staging after
their I/O forwarding works. Xuechen Zhang continued their server-side coordination work. Huaiming Song from
XianHe Sun's group proposed a very interesting strategy to coordinate requests on the server side to accelerate
the average finish time of each request. This is the first SC paper since 2008 from XianHe Sun's group.&lt;/p&gt;

&lt;p&gt;IBM Blue Gene/Q began to deploy in different instituations like &lt;a href=&quot;Lawrence%20Livermore%20National%20Laboratory&quot;&gt;LLNL&lt;/a&gt;. So
there are papers talking about it got published easily:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Dong Chen, et al.] The IBM Blue Gene/Q interconnection network and message unit&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Also, we can see that there is a new kind of papers showing in this year: Querying large scale data. It is well
recognized that querying large scale data is another big problem once we are able to generate huge amount of
data with much more compute resources.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Kalin Kanov, et al.] I/O streaming evaluation of batch queries for data-intensive computational turbulence&lt;/li&gt;
&lt;li&gt;[Jerry Chou, et al.] Parallel index and query for large scale data analysis&lt;/li&gt;
&lt;li&gt;[Siriram Lakshminarasimhan, et al.] ISABELA-QA: query-driven analytics with ISABELA-compressed extreme-scale scientific data&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Besides, the MapReduce model hit the scientific guys in this year, we have a whole session named &quot;MapReduce and network Qos&quot;
in this year, we can see some interesting works here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Joe B. Buck, et al.] SciHadoop: array-based query processing in Hadoop&lt;/li&gt;
&lt;li&gt;[Wittawat Tantisiriroj, et al.] On the duality of data-intensive file system design: reconciling HDFS and PVFS&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;There is not any research paper bout flash-based storage systems, but there are still papers about other
possible new devices that can be the storage materials in future, for example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Xiaojian Wu, et al.] SCMFS: a file system for storage class memory&lt;/li&gt;
&lt;li&gt;[Michael Frasca, et al.] Virtual I/O caching: dynamic storage cache management for concurrent workloads&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It seems that people are not satisfied with the flash-based storage systems. They are asking for a higher
speed way to process data. Memory may be a good way here. There is several very interesting papers that i should check out in this year:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Peter M. Kogge, et al.] Using the TOP500 to trace and project technology and architecture trends&lt;/li&gt;
&lt;li&gt;[Mario Lassnig, et al.] A similarity measure for time, frequency, and dependencies in large-scale workloads&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Exascale has been a topic here, we can see it in these papers:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Jen-Hsun Huang] SC 2011 Keynote&lt;/li&gt;
&lt;li&gt;[Osman Sarood, et al.] A 'cool' load balancer for parallel applications&lt;/li&gt;
&lt;li&gt;[Kurt Ferreira, et al.] Evaluating the viability of process replication reliability for exascale systems&lt;/li&gt;
&lt;li&gt;[Sheng Li, et al.] System implications of memory reliability in exascale computing&lt;/li&gt;
&lt;li&gt;[Abhinav Bhatele, et al.] Avoiding hot-spots on two-level direct networks&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This year's proceedings can be retrieved here: &lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2063384&amp;amp;picked=prox&quot;&gt;SC 2011 Proceeding&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In 2012, graph algorithm in HPC is a very hot topic: actually we can see two whole sessions about this topic:
&lt;em&gt;Breadth first search&lt;/em&gt; and &lt;em&gt;Graph algorithms&lt;/em&gt;. Graph is actually very hot in that year, in other conferences,
it also occupied a lot. Besides, we can see some interesting sessions too: &lt;em&gt;Big data&lt;/em&gt;, &lt;em&gt;Grid/Cloud networking&lt;/em&gt;,
&lt;em&gt;Cloud Computing&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In HPC I/O society, the most significant changing is: first, I/O is more important than ever before:
we actually have three sessions about I/O or storages: &lt;em&gt;Analysis of I/O and storage&lt;/em&gt;, &lt;em&gt;Optimizing I/O for analytics&lt;/em&gt;, &lt;em&gt;Visualization and analysis of massive data sets&lt;/em&gt;.
Second, We are shifting from write-side optimization to the read-side or analysis side optimization. We can see
the paper list here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Robert Henschel, et al.] Demonstrating lustre over a 100Gbps wide area network of 3,500km&lt;/li&gt;
&lt;li&gt;[Dirk Meister, et al.] A study on data deduplication in HPC storage systems&lt;/li&gt;
&lt;li&gt;[Bing Xie, et al.] Characterizing output bottlenecks in a supercomputer&lt;/li&gt;
&lt;li&gt;[John Jenkins, et al.] Byte-precision level of detail processing for variable precision analytics&lt;/li&gt;
&lt;li&gt;[Janine Benneth, et al.] Combining in-situ and in-transit processing to enable extreme-scale scientific analysis&lt;/li&gt;
&lt;li&gt;[Sidharth Kumar, et al.] Efficient data restructuring and aggregation for I/O acceleration in PIDX.&lt;/li&gt;
&lt;li&gt;[Surendra Byna, et al.] Parallel I/O, analysis, and visualization of a trillion particle simulation&lt;/li&gt;
&lt;li&gt;[Kalin Kanov, et al.] Data-intensive spatial filtering in large numerical simulation datasets&lt;/li&gt;
&lt;li&gt;[Boonthanome Nouanesengsy, et al.] Parallel particle advection and FTLE computation for time-varying flow fields&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As they introduced lots of new stuff in this year SC, there is not enough room for flash-based storage there now.
In fact, as we have said before, there is an obviuos shift from flash-based storage to memory based storage. In fact,
based on current research, memory devices especially the new storage level memory devices have significant advantages
compared with flash. Actually in this year, there are several papers in sessions about the memory storage:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Donghong Wu, et al.] RAMZzz: rank-aware dram power management with dynamic migrations and demotions&lt;/li&gt;
&lt;li&gt;[Sheng Li, et al.] MAGE: adaptive granularity and ECC for resilient and power efficient memory systems&lt;/li&gt;
&lt;li&gt;[I-Hsin Chung, et al.] Application data prefetching on the IBM blue gene/Q supercomputer&lt;/li&gt;
&lt;li&gt;[Lluc Alvarez, et al.] Hardware-software coherence protocol for the coexistence of caches and local memories&lt;/li&gt;
&lt;li&gt;[Martin Schindewolf, et al.] What scientific applications can benefit from hardware transactional memory?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Although, there are still works on current best high performance machines, it is clear that &quot;Petaflop&quot; machines are not our first concern any more, we want to know what will happen
in next generation &quot;Exascale&quot; machines.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Dong Chen, et al.] Looking under the hood of the IBM blue gene/Q network&lt;/li&gt;
&lt;li&gt;[I-Hsin Chung, et al.] Application data prefetching on the IBM blue gene/Q supercomputer&lt;/li&gt;
&lt;li&gt;[Jun Doi] Peta-scale lattice quantum chromodynamics on a blue gene/Q supercomputer&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This year's proceedings can be retrieved here: &lt;a href=&quot;http://www.informatik.uni-trier.de/~ley/db/conf/sc/sc2012.html&quot;&gt;SC 2012 Proceeding&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In 2013, things become more resonable: there are not so many &lt;em&gt;big data&lt;/em&gt;, &lt;em&gt;cloud&lt;/em&gt; things. The very very hot
graph algorithm also reduces into a shared session with sorting algorithm. People began to really care
about how the future memory systems for such many cores systems. In fact, we have two sessions about this
topic: &lt;em&gt;Memory Hierarchy&lt;/em&gt; and &lt;em&gt;Memory resilience&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Richard M. Yoo, et al.] Performance evaluation of Intel® transactional synchronization extensions for high-performance computing&lt;/li&gt;
&lt;li&gt;[Jongsoo Park, et al.] Location-aware cache management for many-core processors with deep cache hierarchy&lt;/li&gt;
&lt;li&gt;[Doe Hyun Yoon, et al.] Practical nonvolatile multilevel-cell phase change memory&lt;/li&gt;
&lt;li&gt;[Vilas Sridharan, et al.] Feng shui of supercomputer memory: positional effects in DRAM and SRAM faults&lt;/li&gt;
&lt;li&gt;[Bharan Giridhar, et al.] Exploring DRAM organizations for energy-efficient and resilient exascale memories&lt;/li&gt;
&lt;li&gt;[Xun Jian, et al.] Low-power, low-storage-overhead chipkill correct via multi-line error correction&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Besides memory, we can see that the I/O is still the core challenge: we can see four sessions talking about
this problem: &lt;em&gt;IO tuning&lt;/em&gt;, &lt;em&gt;Optimizing data movement&lt;/em&gt;, &lt;em&gt;In-situ data analytics and reduction&lt;/em&gt;, and &lt;em&gt;Improving large-scale computation and data resources&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[Sidharth Kumar, et al.] Characterization and modeling of PIDX parallel I/O for performance optimization&lt;/li&gt;
&lt;li&gt;[Babak Behzad, et al.] Taming parallel I/O complexity with auto-tuning&lt;/li&gt;
&lt;li&gt;[Da Zheng, et al.] Toward millions of file system IOPS on low-cost, commodity hardware&lt;/li&gt;
&lt;li&gt;[Joe B. Buck, et al.] SIDR: structure-aware intelligent data routing in Hadoop&lt;/li&gt;
&lt;li&gt;[Tong jin, et al.] Using cross-layer adaptations for dynamic data management in large scale coupled scientific workflows&lt;/li&gt;
&lt;li&gt;[Myoungsoo Jung, et al.] Exploring the future of out-of-core computing with compute-local non-volatile memory&lt;/li&gt;
&lt;li&gt;[Daniel E. Laney, et al.]  Assessing the effects of data compression in simulations using physically motivated metrics.&lt;/li&gt;
&lt;li&gt;[Marc Gamell, et al.]  Exploring power behaviors and trade-offs of in-situ data analytics&lt;/li&gt;
&lt;li&gt;[Fang Zheng, et al.] GoldRush: resource efficient in situ scientific data analytics using fine-grained interference aware execution&lt;/li&gt;
&lt;li&gt;[Eli Dart, et al.] The Science DMZ: a network design pattern for data-intensive science.&lt;/li&gt;
&lt;li&gt;[James C. Browne, et al.] Enabling comprehensive data-driven system management for large computational facilities.&lt;/li&gt;
&lt;li&gt;[Jay F. Lofstead, et al.] Insights for exascale IO APIs from building a petascale IO API.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Note that there are lots of applications are actually using &quot;Petaflop&quot; machines now. Also, there is an interesting
research paper from Jay and Rob on the insights for exascale I/O APIs from existing petascale I/O. This is
kind of experince paper which includes many tips about next steps.&lt;/p&gt;

&lt;p&gt;This year's proceedings can be retrieved here: &lt;a href=&quot;http://www.informatik.uni-trier.de/~ley/db/conf/sc/sc2013.html&quot;&gt;SC 2013 Proceeding&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Happy New Year</title>
   <link href="http://daidong.github.io/2014/01/01/Happy-New-Year.html"/>
   <updated>2014-01-01T00:00:00-06:00</updated>
   <id>http://daidong.github.io/blog/2014/01/01/Happy-New-Year</id>
   <content type="html">&lt;h1&gt;Happy New Year&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;1 Jan 2014 - Lubbock&lt;/p&gt;


&lt;p&gt;&lt;img src=&quot;/images/IMG_20131228.jpg&quot; alt=&quot;StateHW114&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Happy new year everyone!&lt;/p&gt;

&lt;p&gt;On this remarkable day, i wrote this blog for new year's resolutions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Continuously write this blog in this year. Fill it with what i found and interested in.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create one site that others need and i want. I have been way behind my schedule to build such a site. It is time to work on it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Try harder to write paper and proposal. Do not make too many wishes and really work hard on problem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Read more books.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Learn new lanuages: Python and Go.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;There are more and more things you need to consider. No longer a baby now, please take care yourself and your family.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Dallas Travel</title>
   <link href="http://daidong.github.io/2013/12/26/Dallas-Travel.html"/>
   <updated>2013-12-26T00:00:00-06:00</updated>
   <id>http://daidong.github.io/blog/2013/12/26/Dallas-Travel</id>
   <content type="html">&lt;h1&gt;Dallas Travel&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;26 Dec 2013 - Dallas&lt;/p&gt;


&lt;p&gt;We planed a travel to Dallas for two days this month. Here is some interesting photos we took there.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Go! MAVS&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/dallas/IMG_20131226.jpg&quot; alt=&quot;gomavs&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Nice Le Meridien Dallas&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/dallas/IMG_20131226_234834.jpg&quot; alt=&quot;hotel&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Empty American Airline Center&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/dallas/IMG_20131226_221016.jpg&quot; alt=&quot;aac&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Red Museum&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/dallas/redmuseum.jpg&quot; alt=&quot;rm&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;John F. Kennedy Memorial Plaza&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/dallas/jfkplaza.jpg&quot; alt=&quot;jfk&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;John F. Kennedy Memorial Plaza 2&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/dallas/jfk.jpg&quot; alt=&quot;jfk2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Land Mark where JFK was killed&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/dallas/landmark.jpg&quot; alt=&quot;landmark&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;West End Historical Distinct&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;img src=&quot;/images/dallas/westend.jpg&quot; alt=&quot;westend&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, we can not forget to have a nice Chinese food!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dallas/food.jpg&quot; alt=&quot;food&quot; /&gt;&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Distributed Filesystem Overview</title>
   <link href="http://daidong.github.io/2013/07/23/distributed-filesystem-overview.html"/>
   <updated>2013-07-23T00:00:00-05:00</updated>
   <id>http://daidong.github.io/blog/2013/07/23/distributed-filesystem-overview</id>
   <content type="html">&lt;h1&gt;Distributed Filesystem Overview&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;23 Jul 2013 - Lubbock&lt;/p&gt;


&lt;p&gt;Distributed Filesystems can be divided into four categories acccording to WIKI pages. &lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_file_systems&quot;&gt;Distributed Filesystem&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Distributed fault-tolerant file systems&lt;/li&gt;
&lt;li&gt;Distributed parallel file systems&lt;/li&gt;
&lt;li&gt;Distributed parallel fault-tolerant file systems&lt;/li&gt;
&lt;li&gt;Distributed peer-to-peer file systems&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For distributed fault-tolerant file systems, it replicates data between nodes (between servers or servers/clients) for high availability and offline (disconnected) operation.&lt;/p&gt;

&lt;p&gt;Distributed parallel file systems stripe data over multiple servers for high performance. They are normally used in high-performance computing (HPC). Some of the distributed parallel file systems use object storage device (OSD) (In Lustre called OST) for chunks of data together with centralized metadata servers. This filesystems include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Fraunhofer Parallel File System (FhGFS) from the Fraunhofer Society Competence Center for High Performance Computing. Available free of charge for Linux under a proprietary license. (High availability features are on the roadmap.)&lt;/li&gt;
&lt;li&gt;Parallel Virtual File System (PVFS, PVFS2, OrangeFS). Developed to store virtual system images, with a focus on non shared writing optimizations. Available for Linux under GPL.&lt;/li&gt;
&lt;li&gt;Starfish is a POSIX-compatible, N-way redundant file system created by Digital Bazaar Inc. and published under a pseudo open source license. Available for Linux and Mac OS. Windows support is available via Samba.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Distributed parallel fault-tolerant file systems, which also are parallel and fault tolerant, stripe and replicate data over multiple servers for high performance and to maintain data integrity. Even if a server fails no data is lost. The file systems are used in both high-performance computing (HPC) and high-availability clusters. GFS and HDFS belong to this category.&lt;/p&gt;

&lt;h2&gt;PVFS (Parallel Virtual File System)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;The first version of PVFS was released in 2003, developed by several scientists in Argonne lab and other institutions. It was designed for use in large scale cluster computing. It focused on providing higher speed on larger amount of data sets. The client library provides for high performance access via Message Passing Interface (MPI).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In 1996, Rob Ross etc. published a paper in HPDC(&lt;a href=&quot;http://www.parl.clemson.edu/pvfs/hpdc96/hpdc96.html&quot;&gt;Implementation and Performance of a Parallel File System for High Performance Distributed Applications&lt;/a&gt;) to demonstrate how PVFS was designed and implemented. In this paper, he defined PVFS as a stream-based file system that provides disk striping across multiple nodes in distributed parallel computers and file partitions among tasks in a parallel program.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>200 Papers on High Performance I/O (Part 1. SC)</title>
   <link href="http://daidong.github.io/2013/06/24/hpc-200-papers-sc.html"/>
   <updated>2013-06-24T00:00:00-05:00</updated>
   <id>http://daidong.github.io/blog/2013/06/24/hpc-200-papers-sc</id>
   <content type="html">&lt;h1&gt;200 Papers on High Performance I/O (Part 1. SC)&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;24 Jun 2013 - Suzhou&lt;/p&gt;


&lt;h2&gt;SC 2012: Salt Lake City, UT, USA&lt;/h2&gt;

&lt;p&gt;In &lt;strong&gt;&lt;a href=&quot;http://www.informatik.uni-trier.de/~ley/db/conf/sc/sc2012.html&quot;&gt;SC 2012&lt;/a&gt;&lt;/strong&gt;, there were 105 papers accepted by the main conference, parts of them are relevant with I/O systems. Here, we firstly list all the relative papers, then read/blog these papers.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SESSION Analysis of I/O and storage&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2389005&quot;&gt;Demonstrating lustre over a 100Gbps wide area network of 3,500km&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://people.ac.upc.edu/toni/Toni_web_site/Toni_Cortes_-_Conferences_files/sc2012_hpc_dedup.pdf&quot;&gt;A study on data deduplication in HPC storage systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://info.ornl.gov/sites/publications/Files/Pub41514.pdf&quot;&gt;Characterizing output bottlenecks in a supercomputer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;SESSION Optimizing I/O for analytics&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6468527&quot;&gt;Byte-precision level of detail processing for variable precision analytics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.janinebennett.org/index_files/sc12.pdf&quot;&gt;Combining in-situ and in-transit processing to enable extreme-scale scientific analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.researchgate.net/publication/233733599_Efficient_Data_Restructuring_and_Aggregation_for_IO_Acceleration_in_PIDX/file/79e4150bd160e716fb.pdf&quot;&gt;Efficient data restructuring and aggregation for I/O acceleration in PIDX&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;SESSION Visualization and analysis of massive data sets&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2389077&quot;&gt;Parallel I/O, analysis, and visualization of a trillion particle simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://lazar.cs.jhu.edu/Site/Publications_files/sc2012.pdf&quot;&gt;Data-intensive spatial filtering in large numerical simulation datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://dl.acm.org/citation.cfm?id=2389079&quot;&gt;Parallel particle advection and FTLE computation for time-varying flow fields&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;SESSION Big data&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.mcs.anl.gov/~wozniak/papers/AMFS_2012.pdf&quot;&gt;Design and analysis of data management in scalable parallel scripting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.ssrc.ucsc.edu/Papers/adams-sc12.pdf&quot;&gt;Usage behavior of a large-scale scientific archive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6468454&quot;&gt;On distributed file tree walk of parallel file systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;SESSION Locality in programming models and runtimes&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;SC 2011: Seattle WA, USA&lt;/h2&gt;

&lt;p&gt;In &lt;strong&gt;&lt;a href=&quot;http://www.informatik.uni-trier.de/~ley/db/conf/sc/sc2011.html&quot;&gt;SC 2011&lt;/a&gt;&lt;/strong&gt;, there were 77 papers accepted by the main conference, parts of them are relevant with I/O systems:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SESSION Coordinating I/O&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Server-side I/O coordination for parallel file systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;QoS support for end users of I/O-intensive applications using shared storage systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Topology-aware data movement and staging for I/O acceleration on Blue Gene/P supercomputing systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;SESSION Querying large scale data&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;I/O streaming evaluation of batch queries for data-intensive computational turbulence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Parallel index and query for large scale data analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;ISABELA-QA: query-driven analytics with ISABELA-compressed extreme-scale scientific data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;SESSION Storage and memory&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Virtual I/O caching: dynamic storage cache management for concurrent workloads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;SCMFS: a file system for storage class memory&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Optimized pre-copy live migration for memory intensive applications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;em&gt;SESSION NOTHING&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;SC 2010: New Orleans LA, USA&lt;/h2&gt;

&lt;p&gt;In &lt;strong&gt;&lt;a href=&quot;http://www.informatik.uni-trier.de/~ley/db/conf/sc/sc2010.html&quot;&gt;SC 2010&lt;/a&gt;&lt;/strong&gt;, there were xx papers accepted by the main conference, parts of them are relevant with I/O systems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Understanding the Impact of Emerging Non-Volatile Memories on High-Performance, IO-Intensive Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Accelerating I/O Forwarding in IBM Blue Gene/P Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Managing Variability in the IO Performance of Petascale Storage Systems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;IOrchestrator: Improving the Performance of Multi-node I/O Systems via Inter-Server Coordination&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;&quot;&gt;Automatic Run-time Parallelization and Transformation of I/O&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Ready To Graduate From Phd.</title>
   <link href="http://daidong.github.io/2013/06/23/ready-to-graduate-from-phd.html"/>
   <updated>2013-06-23T00:00:00-05:00</updated>
   <id>http://daidong.github.io/blog/2013/06/23/ready-to-graduate-from-phd</id>
   <content type="html">&lt;h1&gt;Ready To Graduate From Phd.&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;23 Jul 2013 - Hefei&lt;/p&gt;


&lt;p&gt;Yesterday, the school organized a great graduation ceremony for all the graduate students this year. My wife also came to Hefei to participate this ceremony as a compensation for i missed my bachelor graduation ceremony in 2006. Everything is so great, especially, when school president Hou says 'congratulation' to me, i feel the tears in my eyes.&lt;/p&gt;

&lt;p&gt;Five years Phd. changes a lot of things. First of all, i have become a husband now, although i have not known how to become a perfect husband, but i do work really hard on this &lt;strong&gt;job&lt;/strong&gt;. After this, i have finally found my dream at the very end of these five years. I would rather to be a not-so-rich innovator other than a millionaire worker; I would rather to be a professor other than a CXO in company; I would rather to think about problems in next 50~100 years other than solving people's diary problem. &lt;strong&gt;I want one day i can help to discover the intelligence and build the final artificial intelligence.&lt;/strong&gt;Also, there are side changes during last five years. My parents are older and older, this fact always reminds me that i should try harder to make their hopes on me become reality. And in last three years, i lost two close relatives, This really gives me a sad impression about life. No body can avoid death, so, i hope my life is not just a waste of time to pursue money, comfort, or fame.&lt;/p&gt;

&lt;p&gt;In next two years, i will have a post-doctoral experience in U.S. Living in a complete strange country is difficult and tough for me and my wife, but it would be also benefits us greatly as long as we can support each other with respect and love.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Open Source (Almost) Everything</title>
   <link href="http://daidong.github.io/2011/11/22/open-source-everything.html"/>
   <updated>2011-11-22T00:00:00-06:00</updated>
   <id>http://daidong.github.io/blog/2011/11/22/open-source-everything</id>
   <content type="html">&lt;h1&gt;Open Source (Almost) Everything&lt;/h1&gt;

&lt;p class=&quot;meta&quot;&gt;22 Nov 2011 - San Francisco&lt;/p&gt;


&lt;p&gt;When Chris and I first started working on GitHub in late 2007, we split the work into two parts. Chris worked on the Rails app and I worked on Grit, the first ever Git bindings for Ruby. After six months of development, Grit had become complete enough to power GitHub during our public launch of the site and we were faced with an interesting question:&lt;/p&gt;

&lt;p&gt;Should we open source Grit or keep it proprietary?&lt;/p&gt;

&lt;p&gt;Keeping it private would provide a higher hurdle for competing Ruby-based Git hosting sites, giving us an advantage. Open sourcing it would mean thousands of people worldwide could use it to build interesting Git tools, creating an even more vibrant Git ecosystem.&lt;/p&gt;

&lt;p&gt;After a small amount of debate we decided to open source Grit. I don't recall the specifics of the conversation but that decision nearly four years ago has led to what I think is one of our most important core values: open source (almost) everything.&lt;/p&gt;

&lt;h2&gt;Why is it awesome to open source (almost) everything?&lt;/h2&gt;

&lt;p&gt;If you do it right, open sourcing code is &lt;strong&gt;great advertising&lt;/strong&gt; for you and your company. At GitHub we like to talk publicly about libraries and systems we've written that are still closed but destined to become open source. This technique has several advantages. It helps determine what to open source and how much care we should put into a launch. We recently open sourced Hubot, our chat bot, to widespread delight. Within two days it had 500 watchers on GitHub and 409 upvotes on Hacker News. This translates into goodwill for GitHub and more superfans than ever before.&lt;/p&gt;

&lt;p&gt;If your code is popular enough to attract outside contributions, you will have created a &lt;strong&gt;force multiplier&lt;/strong&gt; that helps you get more work done faster and cheaper. More users means more use cases being explored which means more robust code. Our very own &lt;a href=&quot;https://github.com/defunkt/resque&quot;&gt;resque&lt;/a&gt; has been improved by 115 different individuals outside the company, with hundreds more providing 3rd-party plugins that extend resque's functionality. Every bug fix and feature that you merge is time saved and customer frustration avoided.&lt;/p&gt;

&lt;p&gt;Smart people like to hang out with other smart people. Smart developers like to hang out with smart code. When you open source useful code, you &lt;strong&gt;attract talent&lt;/strong&gt;. Every time a talented developer cracks open the code to one of your projects, you win. I've had many great conversations at tech conferences about my open source code. Some of these encounters have led to ideas that directly resulted in better solutions to problems I was having with my projects. In an industry with such a huge range of creativity and productivity between developers, the right eyeballs on your code can make a big difference.&lt;/p&gt;

&lt;p&gt;If you're hiring, &lt;strong&gt;the best technical interview possible&lt;/strong&gt; is the one you don't have to do because the candidate is already kicking ass on one of your open source projects. Once technical excellence has been established in this way, all that remains is to verify cultural fit and convince that person to come work for you. If they're passionate about the open source code they've been writing, and you're the kind of company that cares about well-crafted code (which clearly you are), that should be simple! We hired &lt;a href=&quot;https://github.com/tanoku&quot;&gt;Vicent Martí&lt;/a&gt; after we saw him doing stellar work on &lt;a href=&quot;https://github.com/libgit2/libgit2&quot;&gt;libgit2&lt;/a&gt;, a project we're spearheading at GitHub to extract core Git functionality into a standalone C library. No technical interview was necessary, Vicent had already proven his skills via open source.&lt;/p&gt;

&lt;p&gt;Once you've hired all those great people through their contributions, dedication to open source code is an amazingly effective way to &lt;strong&gt;retain that talent&lt;/strong&gt;. Let's face it, great developers can take their pick of jobs right now. These same developers know the value of coding in the open and will want to build up a portfolio of projects they can show off to their friends and potential future employers. That's right, a paradox! In order to keep a killer developer happy, you have to help them become more attractive to other employers. But that's ok, because that's exactly the kind of developer you want to have working for you. So relax and let them work on open source or they'll go somewhere else where they can.&lt;/p&gt;

&lt;p&gt;When I start a new project, I assume it will eventually be open sourced (even if it's unlikely). This mindset leads to &lt;strong&gt;effortless modularization&lt;/strong&gt;. If you think about how other people outside your company might use your code, you become much less likely to bake in proprietary configuration details or tightly coupled interfaces. This, in turn, leads to cleaner, more maintainable code. Even internal code should pretend to be open source code.&lt;/p&gt;

&lt;p&gt;Have you ever written an amazing library or tool at one job and then left to join another company only to rewrite that code or remain miserable in its absence? I have, and it sucks. By getting code out in the public we can drastically &lt;strong&gt;reduce duplication of effort&lt;/strong&gt;. Less duplication means more work towards things that matter.&lt;/p&gt;

&lt;p&gt;Lastly, &lt;strong&gt;it's the right thing to do&lt;/strong&gt;. It's almost impossible to do anything these days without directly or indirectly executing huge amounts of open source code. If you use the internet, you're using open source. That code represents millions of man-hours of time that has been spent and then given away so that everyone may benefit. We all enjoy the benefits of open source software, and I believe we are all morally obligated to give back to that community. If software is an ocean, then open source is the rising tide that raises all ships.&lt;/p&gt;

&lt;h2&gt;Ok, then what shouldn't I open source?&lt;/h2&gt;

&lt;p&gt;That's easy. Don't open source anything that represents core business value.&lt;/p&gt;

&lt;p&gt;Here are some examples of what we don't open source and why:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Core GitHub Rails app (easier to sell when closed)&lt;/li&gt;
&lt;li&gt;The Jobs Sinatra app (specially crafted integration with github.com)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Here are some examples of things we do open source and why:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Grit (general purpose Git bindings, useful for building many tools)&lt;/li&gt;
&lt;li&gt;Ernie (general purpose BERT-RPC server)&lt;/li&gt;
&lt;li&gt;Resque (general purpose job processing)&lt;/li&gt;
&lt;li&gt;Jekyll (general purpose static site generator)&lt;/li&gt;
&lt;li&gt;Gollum (general purpose wiki app)&lt;/li&gt;
&lt;li&gt;Hubot (general purpose chat bot)&lt;/li&gt;
&lt;li&gt;Charlock_Holmes (general purpose character encoding detection)&lt;/li&gt;
&lt;li&gt;Albino (general purpose syntax highlighting)&lt;/li&gt;
&lt;li&gt;Linguist (general purpose filetype detection)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Notice that everything we keep closed has specific business value that could be compromised by giving it away to our competitors. Everything we open is a general purpose tool that can be used by all kinds of people and companies to build all kinds of things.&lt;/p&gt;

&lt;h2&gt;What is the One True License?&lt;/h2&gt;

&lt;p&gt;I prefer the MIT license and almost everything we open source at GitHub carries this license. I love this license for several reasons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It's short. Anyone can read this license and understand exactly what it means without wasting a bunch of money consulting high-octane lawyers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Enough protection is offered to be relatively sure you won't sue me if something goes wrong when you use my code.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Everyone understands the legal implications of the MIT license. Weird licenses like the WTFPL and the Beer license pretend to be the &quot;ultimate in free licenses&quot; but utterly fail at this goal. These fringe licenses are too vague and unenforceable to be acceptable for use in some companies. On the other side, the GPL is too restrictive and dogmatic to be usable in many cases. I want everyone to benefit from my code. Everyone. That's what Open should mean, and that's what Free should mean.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;Rad, how do I get started?&lt;/h2&gt;

&lt;p&gt;Easy, just flip that switch on your GitHub repository from private to public and tell the world about your software via your blog, Twitter, Hacker News, and over beers at your local pub. Then sit back, relax, and enjoy being part of something big.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://news.ycombinator.com/item?id=3267432&quot;&gt;Discuss this post on Hacker News&lt;/a&gt;&lt;/p&gt;
</content>
 </entry>
 
 
</feed>